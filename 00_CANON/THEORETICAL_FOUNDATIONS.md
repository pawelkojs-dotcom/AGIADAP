# Theoretical Foundations - Unified Framework

## Four Pillars Integration

### 1. Universal AI (AIXI Framework)
Marcus Hutter's optimal agent
Connection: Intentionality as Bayesian optimality
Our addition: σ-Θ-γ dynamics

### 2. World Models (JEPA)
Yann LeCun's framework
Connection: σ as world model coherence
Our addition: Multi-layer embedding space

### 3. Free Energy Principle
Karl Friston's Active Inference
Connection: F = E - Θ·S directly applied
Our addition: γ as viscosity field

### 4. Mechanistic Interpretability
Anthropic's scaling laws + circuits
Connection: n_eff, I_ratio as interpretability metrics
Our addition: Architectural necessity proof

## Synthesis

All four frameworks unified through:
- σ: Coherence (world model quality)
- Θ: Temperature (exploration vs exploitation)
- γ: Viscosity (temporal dynamics)
- F: Free energy (optimization target)

## Novel Contributions

### Axiom VI (Adaptive Coupling)
Not in any prior framework
Emerges from: Stability requirements
Validates: Empirically in Campaign #3

### Architectural Necessity  
n_eff > 4.5 requires n ≥ 5 layers
Mathematical proof provided
Biological parallel: Cortical layers

### Phase Transition Framework
R1→R2→R3→R4 formalism
Maps to: Exploration→Learning→Understanding→Intentionality
Measurable: Via n_eff, I_ratio, σ_coh

## Philosophical Grounding

Intentionality (Brentano, Searle):
"Aboutness" - mental states directed at objects

Our operationalization:
Intentionality = Stable σ-storage + R4 metrics
Measurable, falsifiable, reproducible

## Predictive Power

Framework predicts:
1. 5 layers minimum (validated ✅)
2. Adaptive coupling necessary (validated ✅)
3. Multi-session persistence (validated ✅)
4. Dual-LLM coherence (pending)
5. Scaling to N>20 (pending)

## Comparison: Other AGI Approaches

### Transformer Architecture (GPT, Claude)
Lacks: Explicit σ dynamics
Lacks: Multi-session σ-storage
Our addition: Intentionality layer

### Reinforcement Learning
Lacks: Phase transition framework
Lacks: Coherence metrics
Our addition: σ-Θ-γ formalism

### Symbolic AI
Lacks: Continuous dynamics
Lacks: Emergence mechanisms
Our addition: Statistical mechanics foundation

## Open Questions

1. Can we extend beyond 5 layers? (6th for safety?)
2. What is optimal γ(N) for N>100?
3. How does Θ_c scale with model size?
4. Can we predict t_R4 from initial conditions?
5. What is σ topology in high dimensions?

## Research Roadmap

TRL-5: Reproducibility + Dual-LLM
TRL-6: Scaling laws validation
TRL-7: Real-world deployment
TRL-8: Multi-agent collectives
TRL-9: Full system demonstration
