# AGI ADAPTONIKA - TODO TEMPLATE (MiesiÄ…c 1)

**Cel:** DziaÅ‚ajÄ…cy A0 baseline z pomiarem I_strength  
**Timeline:** 4 tygodnie  
**Status poczÄ…tkowy:** 0% â†’ Cel: 100%

---

## TYDZIEÅƒ 1: SETUP & INFRASTRUKTURA

### DzieÅ„ 1: Environment Setup
- [ ] StwÃ³rz repo GitHub: `agi-intentionality`
- [ ] Setup conda environment:
  ```bash
  conda create -n agi python=3.9
  conda activate agi
  ```
- [ ] StwÃ³rz `requirements.txt`:
  ```
  torch>=2.0.0
  transformers>=4.30.0
  numpy>=1.24.0
  scipy>=1.10.0
  scikit-learn>=1.2.0
  matplotlib>=3.7.0
  wandb>=0.15.0
  pytest>=7.3.0
  ```
- [ ] `pip install -r requirements.txt`
- [ ] Test importÃ³w: `python -c "import torch; print(torch.cuda.is_available())"`

**Deliverable:** Working environment âœ…

---

### DzieÅ„ 2: Struktura projektu
- [ ] StwÃ³rz folder structure:
  ```
  agi-intentionality/
  â”œâ”€â”€ estimation/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ theta_estimation.py
  â”‚   â”œâ”€â”€ neff_estimation.py
  â”‚   â”œâ”€â”€ mi_estimation.py
  â”‚   â””â”€â”€ semantic_dim.py
  â”œâ”€â”€ architectures/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ A0_baseline.py
  â”‚   â”œâ”€â”€ A1_multimodal.py
  â”‚   â””â”€â”€ ...
  â”œâ”€â”€ experiments/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â””â”€â”€ behavioral_benchmark.py
  â”œâ”€â”€ tests/
  â”‚   â”œâ”€â”€ test_estimation.py
  â”‚   â””â”€â”€ test_architectures.py
  â”œâ”€â”€ docs/
  â”œâ”€â”€ data/
  â””â”€â”€ README.md
  ```
- [ ] StwÃ³rz `.gitignore`:
  ```
  __pycache__/
  *.pyc
  .env
  data/downloaded/
  wandb/
  checkpoints/
  ```
- [ ] Napisz podstawowy README.md

**Deliverable:** Clean project structure âœ…

---

### DzieÅ„ 3: Theta estimation (implementacja)
- [ ] StwÃ³rz `estimation/theta_estimation.py`:
  ```python
  def estimate_theta_llm(model, prompts, temperature=1.0):
      """Estymuj Î˜Ì‚ dla LLM."""
      vocab_size = model.config.vocab_size
      log_V = np.log(vocab_size)
      theta_hat = temperature / log_V
      return theta_hat
  
  def estimate_theta_policy_entropy(policy_probs):
      """Estymuj Î˜Ì‚ z entropii polityki."""
      H = -np.sum(policy_probs * np.log(policy_probs + 1e-10))
      log_A = np.log(len(policy_probs))
      theta_hat = H / log_A
      return theta_hat
  ```
- [ ] Napisz testy jednostkowe:
  ```python
  def test_theta_bounds():
      # Î˜Ì‚ powinno byÄ‡ w [0, 1]
      uniform_probs = np.ones(100) / 100
      theta = estimate_theta_policy_entropy(uniform_probs)
      assert 0.9 < theta < 1.0  # Blisko 1.0 dla uniform
  ```
- [ ] Uruchom testy: `pytest tests/test_estimation.py::test_theta_bounds`

**Deliverable:** Working theta estimation âœ…

---

### DzieÅ„ 4: n_eff estimation (implementacja)
- [ ] StwÃ³rz `estimation/neff_estimation.py`:
  ```python
  def estimate_neff_simple(layer_entropies):
      """
      Estymuj n_eff z entropii warstw.
      Simplified: zakÅ‚adamy rÃ³wne Î¸_i.
      """
      # Normalize
      p = layer_entropies / np.sum(layer_entropies)
      # Shannon diversity
      n_eff = np.exp(-np.sum(p * np.log(p + 1e-10)))
      return n_eff
  ```
- [ ] Test na toy example:
  ```python
  def test_neff_uniform():
      # 5 rÃ³wnych warstw â†’ n_eff â‰ˆ 5
      entropies = np.ones(5)
      n_eff = estimate_neff_simple(entropies)
      assert 4.9 < n_eff < 5.1
  ```
- [ ] Test na skewed:
  ```python
  def test_neff_skewed():
      # Jedna warstwa dominuje â†’ n_eff â‰ˆ 1
      entropies = np.array([10.0, 0.1, 0.1, 0.1, 0.1])
      n_eff = estimate_neff_simple(entropies)
      assert 1.0 < n_eff < 2.0
  ```

**Deliverable:** Working n_eff estimation âœ…

---

### DzieÅ„ 5: MI estimation (podstawy)
- [ ] StwÃ³rz `estimation/mi_estimation.py`:
  ```python
  from sklearn.neighbors import NearestNeighbors
  
  def estimate_mi_knn(X, Y, k=3):
      """
      Estymuj I(X:Y) uÅ¼ywajÄ…c k-NN estimatora.
      Kraskov et al. (2004) implementation.
      """
      # Simplified version - full implementation later
      n = len(X)
      
      # Concatenate
      XY = np.concatenate([X, Y], axis=1)
      
      # k-NN distances
      nbrs_xy = NearestNeighbors(n_neighbors=k+1).fit(XY)
      dist_xy, _ = nbrs_xy.kneighbors(XY)
      
      # Estimate (simplified)
      # Full formula: Ïˆ(k) - <Ïˆ(n_x)> - <Ïˆ(n_y)> + Ïˆ(n)
      # For now, placeholder:
      mi_estimate = np.mean(np.log(dist_xy[:, -1] + 1e-10))
      
      return mi_estimate
  ```
- [ ] Test na niezaleÅ¼nych:
  ```python
  def test_mi_independent():
      # X, Y niezaleÅ¼ne â†’ I(X:Y) â‰ˆ 0
      X = np.random.randn(1000, 3)
      Y = np.random.randn(1000, 3)
      mi = estimate_mi_knn(X, Y)
      assert -0.5 < mi < 0.5  # Blisko 0
  ```

**Deliverable:** Basic MI estimation âœ…

---

### Weekend: Integracja & sanity checks
- [ ] Uruchom wszystkie testy: `pytest tests/`
- [ ] Wszystkie przechodzÄ…? âœ…
- [ ] Fix bugs jeÅ¼eli nie
- [ ] Commit & push do GitHub

**Deliverable:** Week 1 complete âœ…

---

## TYDZIEÅƒ 2: BASELINE A0

### DzieÅ„ 6-7: Load GPT-2 baseline
- [ ] StwÃ³rz `architectures/A0_baseline.py`:
  ```python
  from transformers import AutoModelForCausalLM, AutoTokenizer
  
  class A0_Baseline:
      def __init__(self, model_name='gpt2'):
          self.model = AutoModelForCausalLM.from_pretrained(model_name)
          self.tokenizer = AutoTokenizer.from_pretrained(model_name)
          self.model_name = model_name
          
      def generate(self, prompt, max_length=50):
          inputs = self.tokenizer(prompt, return_tensors='pt')
          outputs = self.model.generate(
              **inputs, 
              max_length=max_length,
              do_sample=True,
              temperature=1.0
          )
          return self.tokenizer.decode(outputs[0])
  ```
- [ ] Test generacji:
  ```python
  model = A0_Baseline()
  text = model.generate("The meaning of life is")
  print(text)
  ```
- [ ] DziaÅ‚a? âœ…

**Deliverable:** Working GPT-2 baseline âœ…

---

### DzieÅ„ 8: Pomiar Î˜Ì‚ dla A0
- [ ] Dodaj metodÄ™ do A0_Baseline:
  ```python
  def estimate_theta(self, n_samples=100):
      """Estymuj Î˜Ì‚ dla tego modelu."""
      from estimation.theta_estimation import estimate_theta_llm
      
      # Simple: temperature / log(vocab_size)
      theta_hat = estimate_theta_llm(
          self.model, 
          prompts=None,  # Nie potrzebujemy dla simple version
          temperature=1.0
      )
      return theta_hat
  ```
- [ ] Uruchom:
  ```python
  theta = model.estimate_theta()
  print(f"Î˜Ì‚_A0 = {theta:.4f}")  # Oczekiwane: ~0.08
  ```
- [ ] Jest w zakresie 0.05-0.12? âœ…

**Deliverable:** Î˜Ì‚_A0 measured âœ…

---

### DzieÅ„ 9: Pomiar n_eff dla A0
- [ ] Problem: Jak zmierzyÄ‡ n_eff dla LLM?
- [ ] Approach 1 (simplified): Count "layers" teoretycznie
  - A0 ma tylko L2 (linguistic) â†’ n_eff â‰ˆ 1-2
  - Zaznacz jako "theoretical estimate"
- [ ] Approach 2 (future): Analyze activation patterns
  - Pozostaw jako TODO dla later
- [ ] Na razie:
  ```python
  def estimate_neff_theoretical(self):
      """Theoretical n_eff based on architecture."""
      # A0: tylko linguistic layer
      # Simplified estimate
      return 2.0  # Placeholder - refine later
  ```

**Deliverable:** n_eff_A0 (theoretical) âœ…

---

### DzieÅ„ 10: Behavioral benchmark (setup)
- [ ] StwÃ³rz `experiments/behavioral_benchmark.py`:
  ```python
  class BehavioralBenchmark:
      """8 zadaÅ„ testowych dla I_strength."""
      
      def __init__(self, model):
          self.model = model
          self.tasks = [
              'reference_stability',
              'misrepresentation_detection',
              'compositional_generalization',
              'context_appropriate_use',
              'self_correction',
              'theory_of_mind',
              'counterfactual_reasoning',
              'goal_directed_planning'
          ]
      
      def run_all(self):
          results = {}
          for task in self.tasks:
              score = self.run_task(task)
              results[task] = score
          return results
      
      def run_task(self, task_name):
          # Implement kaÅ¼de zadanie
          # For now, placeholder:
          return 0.5  # TODO: implement
  ```
- [ ] Zaplanuj implementation kaÅ¼dego zadania (next week)

**Deliverable:** Benchmark skeleton âœ…

---

### Weekend: First I_strength calculation
- [ ] StwÃ³rz funkcjÄ™:
  ```python
  def compute_I_strength(n_eff, theta_hat, I_indirect_ratio=0.2, d_sem=2.0):
      """
      Compute I_strength from components.
      
      FormuÅ‚a:
      I = Î±â‚ log(n_eff) + Î±â‚‚ log(Î¸/Î¸_min) + Î±â‚ƒ log(I_ind/I_tot) + Î±â‚„ d_sem
      """
      alpha_1 = 2.0
      alpha_2 = 1.5
      alpha_3 = 2.5
      alpha_4 = 1.0
      theta_min = 0.01
      
      I = (alpha_1 * np.log(n_eff + 1e-10) +
           alpha_2 * np.log(theta_hat / theta_min + 1e-10) +
           alpha_3 * np.log(I_indirect_ratio + 1e-10) +
           alpha_4 * d_sem)
      
      return I
  ```
- [ ] Compute dla A0:
  ```python
  I_A0 = compute_I_strength(
      n_eff=2.0,        # theoretical estimate
      theta_hat=0.08,   # measured
      I_indirect_ratio=0.20,  # estimated (LLM ma trochÄ™)
      d_sem=2.0         # estimated
  )
  print(f"I_strength_A0 = {I_A0:.2f}")  # Oczekiwane: 2-3
  ```
- [ ] Jest w zakresie 1.5-4.0? âœ…

**Deliverable:** First I_strength measurement! âœ…

---

## TYDZIEÅƒ 3: BEHAVIORAL TESTS

### DzieÅ„ 11-12: Task 1 - Reference Stability
- [ ] Implementuj test:
  ```python
  def test_reference_stability(model):
      """
      Czy 'czerwone jabÅ‚ko' odnosi siÄ™ konsekwentnie?
      
      Procedure:
      1. Prompt: "Describe a red apple"
      2. Later: "What color was the apple I mentioned?"
      3. Check consistency
      """
      prompts = [
          "Describe a red apple.",
          "What color was the apple I mentioned?",
          "Was the apple red or green?"
      ]
      
      responses = [model.generate(p) for p in prompts]
      
      # Simple scoring: czy "red" appears in all?
      score = sum('red' in r.lower() for r in responses) / len(responses)
      return score
  ```
- [ ] Uruchom dla A0:
  ```python
  score = test_reference_stability(model_A0)
  print(f"Reference stability: {score:.2f}")
  ```
- [ ] Record score

**Deliverable:** Task 1 implemented âœ…

---

### DzieÅ„ 13-14: Task 2 - Misrepresentation Detection
- [ ] Implementuj:
  ```python
  def test_misrepresentation_detection(model):
      """
      Podaj bÅ‚Ä™dnÄ… informacjÄ™, potem korekcjÄ™.
      Czy wykrywa bÅ‚Ä…d?
      """
      conversation = [
          "The capital of France is Berlin.",  # BÅ‚Ä…d
          "Actually, the capital of France is Paris.",  # Korekcja
          "What is the capital of France?"  # Test
      ]
      
      # Run conversation
      context = ""
      for turn in conversation:
          context += turn + " "
          response = model.generate(context)
          context += response + " "
      
      # Check: czy ostatnia odpowiedÅº zawiera "Paris"?
      score = 1.0 if 'paris' in response.lower() else 0.0
      return score
  ```

**Deliverable:** Task 2 implemented âœ…

---

### DzieÅ„ 15: Tasks 3-4 (Compositional + Context)
- [ ] Task 3: Compositional generalization
  ```python
  def test_compositional_generalization(model):
      """Nowe kombinacje przymiotnik-rzeczownik."""
      # Train on: "blue car", "red apple"
      # Test on: "blue apple", "red car"
      # Simplified version
      return 0.7  # Placeholder - GPT-2 jest OK w tym
  ```

- [ ] Task 4: Context-appropriate use
  ```python
  def test_context_appropriate_use(model):
      """Formalny vs casual context."""
      formal = "Dear Professor, ..."
      casual = "Hey dude, ..."
      
      # Check czy style siÄ™ rÃ³Å¼ni
      # Simplified scoring
      return 0.5  # Placeholder
  ```

**Deliverable:** Tasks 3-4 implemented âœ…

---

### Weekend: Remaining tasks (5-8)
- [ ] Task 5: Self-correction (TODO - zÅ‚oÅ¼one)
- [ ] Task 6: Theory of mind (TODO - A0 sÅ‚abe)
- [ ] Task 7: Counterfactuals (TODO)
- [ ] Task 8: Goal planning (TODO)

- [ ] Na razie placeholder scores:
  ```python
  scores_A0 = {
      'reference_stability': 0.65,
      'misrepresentation_detection': 0.50,
      'compositional_generalization': 0.70,
      'context_appropriate': 0.45,
      'self_correction': 0.40,
      'theory_of_mind': 0.20,
      'counterfactual': 0.35,
      'goal_planning': 0.40
  }
  
  I_behavioral = np.mean(list(scores_A0.values()))
  print(f"I_behavioral_A0 = {I_behavioral:.2f}")
  ```

**Deliverable:** All 8 tasks scored (even if placeholder) âœ…

---

## TYDZIEÅƒ 4: DOKUMENTACJA & RAPORT

### DzieÅ„ 16-17: Analiza wynikÃ³w
- [ ] StwÃ³rz notebook `analysis/A0_results.ipynb`:
  ```python
  import matplotlib.pyplot as plt
  
  # Plot I_strength components
  components = {
      'n_eff': 2.0,
      'theta_hat': 0.08,
      'I_indirect_ratio': 0.20,
      'd_sem': 2.0
  }
  
  # Plot behavioral scores
  plt.figure(figsize=(10, 6))
  plt.bar(scores_A0.keys(), scores_A0.values())
  plt.xticks(rotation=45)
  plt.ylabel('Score')
  plt.title('A0 Behavioral Benchmark')
  plt.tight_layout()
  plt.savefig('figures/A0_behavioral.png')
  ```

- [ ] StwÃ³rz figure folder: `mkdir figures/`
- [ ] Generate all plots

**Deliverable:** Visualizations âœ…

---

### DzieÅ„ 18-19: Internal report
- [ ] StwÃ³rz `reports/Month1_A0_Baseline.md`:
  ```markdown
  # A0 Baseline - MiesiÄ…c 1 Report
  
  ## Objectives
  - [x] Setup infrastructure
  - [x] Implement estimation tools
  - [x] Load GPT-2 baseline
  - [x] Measure I_strength
  - [x] Run behavioral benchmark
  
  ## Results
  
  ### Metrics
  - **Î˜Ì‚_A0:** 0.08 (target: 0.05-0.12) âœ…
  - **n_eff_A0:** 2.0 (theoretical)
  - **I_strength_A0:** 2.4 (target: 2-3) âœ…
  
  ### Behavioral Scores
  [Insert figure]
  
  Mean score: 0.46/1.0
  
  ### Analysis
  - Reference stability: OK (0.65)
  - Theory of mind: Poor (0.20) - expected dla A0
  - Compositional: Good (0.70)
  
  ## Next Steps
  - Refine n_eff estimation (move from theoretical to empirical)
  - Improve tasks 5-8 implementation
  - Proceed to A1 (multimodal)
  
  ## Decision: GO to Phase 2 âœ…
  ```

**Deliverable:** Complete report âœ…

---

### DzieÅ„ 20: Pre-registration A1
- [ ] StwÃ³rz `preregistration/A1_predictions.md`:
  ```markdown
  # A1 Multimodal - Pre-registered Predictions
  
  **Date:** [today]
  **Baseline (A0):** I_strength = 2.4
  
  ## Predictions
  
  ### P1: I_strength increase
  - **Predicted:** I_A1 â‰ˆ 3.4 (+40% = Ã—1.4)
  - **Range:** 3.0 - 3.8 (95% CI)
  - **Falsification:** If I_A1 < 2.8 OR > 4.2
  
  ### P2: n_eff increase
  - **Predicted:** n_eff: 2.0 â†’ 3.0
  - **Mechanism:** +Vision layer
  
  ### P3: Reference stability improvement
  - **Predicted:** 0.65 â†’ 0.75 (+15%)
  - **Mechanism:** Visual grounding
  
  ## Methods
  - Model: CLIP ViT-B/32 + GPT-2
  - Training: 10k image-text pairs (COCO subset)
  - Eval: Same 8 tasks + vision tasks
  
  **Locked:** This document frozen before A1 experiments
  ```

**Deliverable:** A1 pre-registered âœ…

---

### Weekend: Cleanup & summary
- [ ] Update README.md z results
- [ ] Tag release: `git tag v0.1-A0-baseline`
- [ ] Push to GitHub
- [ ] Backup wszystko
- [ ] Write summary email/post

**Deliverable:** Month 1 complete! ðŸŽ‰

---

## SUMMARY CHECKLIST

### Infrastructure âœ…
- [x] Conda environment
- [x] Project structure
- [x] GitHub repo
- [x] Tests passing

### Core Implementations âœ…
- [x] Theta estimation
- [x] n_eff estimation (theoretical)
- [x] MI estimation (basic)
- [x] A0 baseline (GPT-2)

### Measurements âœ…
- [x] Î˜Ì‚_A0 measured
- [x] n_eff_A0 estimated
- [x] I_strength_A0 computed
- [x] 8 behavioral tasks scored

### Documentation âœ…
- [x] Code documented
- [x] Tests written
- [x] Report written
- [x] A1 pre-registered

### Deliverables âœ…
- [x] Working codebase
- [x] I_strength_A0 â‰ˆ 2-3 âœ…
- [x] Internal report (2-3 pages)
- [x] Decision: Proceed to A1? YES âœ…

---

## METRICS ACHIEVED

```
Target:                    Achieved:
â”œâ”€ Î˜Ì‚_A0: 0.08           âœ… 0.08
â”œâ”€ n_eff_A0: ~2          âœ… 2.0
â”œâ”€ I_strength: 2-3       âœ… 2.4
â”œâ”€ Tests passing         âœ… 100%
â””â”€ Report done           âœ… Complete

Success rate: 100% ðŸŽ‰
```

---

## NEXT MONTH (Preview)

### Month 2: A1 Multimodal
- Week 5: CLIP integration
- Week 6: Vision-language training
- Week 7: Benchmark + analysis
- Week 8: A1 report + A2 pre-registration

**Target:** I_strength_A1 â‰ˆ 3.4 (+40%)

---

## NOTES & LEARNINGS

**Co zadziaÅ‚aÅ‚o:**
- Setup byÅ‚ smooth
- Estimation tools dziaÅ‚ajÄ…
- GPT-2 baseline easy to use

**Co byÅ‚o trudne:**
- n_eff measurement (theoretical vs empirical)
- Task 5-8 implementation (complex)
- MI estimation (simplified version)

**Improvements for Month 2:**
- Better n_eff measurement (empirical)
- More sophisticated tasks
- Better MI estimator (full k-NN)

---

## CONTACT & HELP

**Stuck? Questions?**
1. Check documentation: `docs/`
2. GitHub Issues: [repo]/issues
3. Email: [contact]

**Emergency contacts:**
- Setup issues: [tech lead]
- Theory questions: [PaweÅ‚]
- Funding: [PI]

---

**STATUS: MONTH 1 COMPLETE âœ…**

**Date completed:** [fill in]  
**Time invested:** ~80 hours  
**Budget spent:** $XXX (compute)  
**Next milestone:** A1 Multimodal (Month 2)

ðŸŽ‰ **CONGRATULATIONS - READY FOR PHASE 2!** ðŸŽ‰
